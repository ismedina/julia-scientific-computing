{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h1> About these notebooks </h1>\n",
    "    <p> If you opened this notebook in Binder, it is running on a server that was launched <b>just</b> for you. Your changes will be reset once server restarts due to inactivity, so don't rely on it for anything you want to last. Likewise, feel free to try and tweak everything you want, since you won't affect the original repository. And <b>if you have never used a Jupyter noteboook: </b> for running a cell just press <tt>Shift + Enter.</tt> </p>\n",
    "    <p>Enjoy!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Introduction\n",
    "\n",
    "The objective of this notebook is to get you started in Julia with a real example: the Gradient Descent algorithm. We have chosen this algorithm because:\n",
    "\n",
    "* **The mathematical problem is simple and ubiquitous**: given a function, find a minimum. \n",
    "* **The algorithm formulation is simple**: just take small leaps following the steepest path down.\n",
    "* **The algorithm admits plenty of variations**: we can play with several variations to explore the Julia lenguage in greater depth \n",
    "\n",
    "\n",
    "# 1.2 Mathematical formulation\n",
    "\n",
    "*Gradient descent* is one of the simplest methods for finding optima. As [Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent) puts it, its core idea is that a differentiable function $f(x)$ decreases fastest if one follows the direction of  $− \\nabla f$ (the direction of greatest slope). Since this is in general only valid close to our starting point, the gradient descent corresponds to the following sequence:\n",
    "\n",
    "$$ x_{n+1} = x_n - \\alpha \\nabla f(x_n) $$\n",
    "\n",
    "where $\\alpha$ is some parameter chosen in such a way that we keep decreasing $f$ in each step (or at least we try so). If $\\alpha$ is too high we could land even farther from the minimum in the next iterate! The resulting sequence $x_n$ will look something like this:\n",
    "\n",
    "![gradient descent gif](figures/gradient_descent.gif)\n",
    "\n",
    "(By the way, we will also learn to generate animations like this. Yay!)\n",
    "\n",
    "\n",
    "\n",
    "# 1.3 The pseudo-code\n",
    "\n",
    "We then want some code that given a function `f`, its derivative or gradient `Df` and a starting point `x`, returns a minimum and the value of `f` at this minimum. The code should look something like this:\n",
    "\n",
    "```julia\n",
    "function gradient_descent(f, Df, x; alpha = 0.1)\n",
    "    while (x is not close enough to a minimum)\n",
    "        x = x - alpha*Df(x)\n",
    "    end\n",
    "    return x, f(x)\n",
    "end\n",
    "```\n",
    "\n",
    "If you are not familiar with the algorithm you may visit [Wikipedia's page](https://en.wikipedia.org/wiki/Gradient_descent#Computational_examples) and check the code in your favorite programming language.\n",
    "\n",
    "Let's learn little by little all we need to code such function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Variables, operations and functions\n",
    "\n",
    "Variable declaration and arithmetic operations won't look challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "s = \"Hello world!\"\n",
    "tup = (x, s) # This is a tuple, a immutable collection of (possibly distinct) elements\n",
    "\n",
    "println(x)   # The function print doesn't add a new line after\n",
    "println(s)\n",
    "println(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2\n",
    "z = y * 5\n",
    "z += 1 # inplace adition\n",
    "\n",
    "print(\"The final value of z is $z\") # The dollar is used for formatting variables into a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard way of defining a function begins with the `function` keyword and must end with an `end`. They return the last computed expression (or whatever follows a `return`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function f(x)\n",
    "    x^2\n",
    "end\n",
    "\n",
    "f(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But sometimes it's simpler to just do:\n",
    "Df(x) = 2*x\n",
    "\n",
    "Df(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`for` loops will be familiar for **Matlab** users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i=1:5\n",
    "    print(i,\" \")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, **Python** users will also feel *almost* at home:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:5\n",
    "    print(i,\" \")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature of Julia is that nested loops can be written very compactly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:2, j in 3:4\n",
    "    println((i,j))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course there are also while-loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "while i <= 5\n",
    "    print(i, \" \")\n",
    "    i+=1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `if` statements won't bring many surprises either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "if a < 0\n",
    "    print(\"a is negative\")\n",
    "elseif a > 0\n",
    "    print(\"a is positive\")\n",
    "else\n",
    "    print(\"a is zero\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>A quick but important warning about scopes</h3>\n",
    "    <p> A frequent cause of frustration in Julia comes from defining some variable inside a loop and trying to accessing it later from outside. Outrageously, this fails to work. Why is that?</p>\n",
    "    <p> Well, it turns out that regarding variables, Julia has a <i>global scope</i> (the basic level of a Jupyter Notebook cell, the REPL,...) and <i>local scopes</i> (a function, a `for` loop...). A variable that is declared inside a local scope cannot be accesed from outside; this behavior is pretty common in other languages when it comes to functions, <b>but</b> in Julia this also extends to loops, which at first can be kind of unexpected. </p>\n",
    "    <p> Check the following code and try to fix it:  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:5\n",
    "    b = i\n",
    "    print(b,\" \")\n",
    "end\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, `if` statements don't introduce any new scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if true\n",
    "    c = 1\n",
    "end\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 A first version of the gradient descent\n",
    "\n",
    "With all these tools we are ready to code a preliminary version of the gradient descend algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient_descent(f, Df, x)\n",
    "    α = 0.1\n",
    "    \n",
    "    for i in 1:100\n",
    "        x = x - α*Df(x)\n",
    "    end\n",
    "    \n",
    "    return (x, f(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(By the way, Julia admits general Unicode as names of variables. For writing the `α` while you are coding you just have to write `\\alpha` and press tab; it will quickly change to `α`)\n",
    "\n",
    "Notice that right now we have a fixed number of iterations, and the learning rate is fixed to `α = 0.1`. However, it kind of does the job; for `f` and `Df` as defined above it yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_opt, f_opt = gradient_descent(f, Df, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the real values for `x_opt` and `f_opt` are 0, the result are really good! \n",
    "\n",
    "We would like now to have a bit more flexibility and robustness. Let us include `α` as an *optional argument*, and also include a *docstring* before the function; this is, some help string that tells us how the algorithm works. This docstring is also what you will see when you press `Shift + Tab` asking for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Second version of the gradient descent, now with alpha as an optional argument\"\"\"\n",
    "function gradient_descent(f, Df, x; α = 0.1)\n",
    "    \n",
    "    for N_iter in 1:100\n",
    "        x = x - α*Df(x)\n",
    "    end\n",
    "    return (x, f(x))\n",
    "end\n",
    "\n",
    "println(\"Testing our implementation:\")\n",
    "println(gradient_descent(f, Df, 1))\n",
    "# Change learning rate\n",
    "\n",
    "println(\"\\nTesting the learning rate:\")\n",
    "println(gradient_descent(f, Df, 1, α=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was expected, diminishing the learning rate reduces the rate of convergence; in this case we are much farther from the minimum. Let's keep improving our `gradient_descent` in the following exercise:\n",
    "\n",
    "#### Exercise 1:\n",
    "\n",
    "Improve the `gradient_descent` function by adding:\n",
    "\n",
    "* *a tolerance and a maximum number of iterations*: stop the iterations only when `Df(x)` is smaller than the tolerance or the maximum number of iterations has been reached (**Hint**: in Julia the \"and\" operator is written `&`).\n",
    "* *progress info*: print the current number of iterations, the value of `x` and that of `f(x)` every 100 iterations (*Hint*: in the second code cell of this notebook you can see how to format variables into text) \n",
    "* *a `verbose` parameter*, to activate or deactivate the showing of progress info.\n",
    "\n",
    "\n",
    "\n",
    "(A solution is given in the `Exercise solutions` notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradient_descent"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Third version of the gradient descent. Stops when the gradient is smaller than `TOL`, \n",
    "    or when the maximum number of iterations `maxiter` has been reached\"\"\"\n",
    "function gradient_descent(f, Df, x; α = 0.1, TOL = 1e-10, maxiter = 1000, verbose = false)\n",
    "    \n",
    "    # Here goes your code\n",
    "    \n",
    "    return (x, f(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter. 100,\tx = 0.13261955589475316,\tf(x) = 0.017587946605721556\n",
      "Iter. 200,\tx = 0.017587946605721556,\tf(x) = 0.00030933586580571244\n",
      "Iter. 300,\tx = 0.002332505667951425,\tf(x) = 5.440582691025523e-6\n",
      "Iter. 400,\tx = 0.00030933586580571244,\tf(x) = 9.568867787376974e-8\n",
      "Iter. 500,\tx = 4.102398514547257e-5,\tf(x) = 1.6829673572159537e-9\n",
      "Iter. 600,\tx = 5.4405826910255245e-6,\tf(x) = 2.959994001788654e-11\n",
      "Iter. 700,\tx = 7.215276602924866e-7,\tf(x) = 5.2060216456715e-13\n",
      "Iter. 800,\tx = 9.56886778737698e-8,\tf(x) = 9.156323073230082e-15\n",
      "Iter. 900,\tx = 1.2690189963775444e-8,\tf(x) = 1.61040921316707e-16\n",
      "Iter. 1000,\tx = 1.6829673572159529e-9,\tf(x) = 2.8323791254544486e-18\n",
      "Iter. 1100,\tx = 2.2319438349934617e-10,\tf(x) = 4.981573282565321e-20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.9049991751351605e-11, 2.4059016908076606e-21)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test it:\n",
    "gradient_descent(f, Df, 1, α = 0.01, maxiter = 10000, verbose = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Multiple dispatch: things start getting different\n",
    "\n",
    "Imagine for a moment that we don't want to care much about the initial point `x`. This may happen for example if we just want to get a local minimum of `f` and not the smallest one. Then it may make sense to define another `gradient descent` that just chooses `x` randomly. We could then make `x` also an optional argument; nevertheless we are going to explore another way that explotes one of Julia's most distinguishing features: *multiple dispatch*. To see how it works, let's consider this alternative version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" A fourth version of gradient descent, where the initial point gets initialized randomly.\n",
    "    Stops when the gradient is smaller than `TOL`, or when the maximum number of iterations \n",
    "    `maxiter` has been reached\"\"\"\n",
    "function gradient_descent(f, Df; α = 0.1, TOL = 1e-10, maxiter = 1000, verbose = false)\n",
    "    x = rand()\n",
    "    gradient_descent(f, Df, x; α = α, TOL = TOL, maxiter = maxiter, verbose = verbose)\n",
    "end\n",
    "\n",
    "# Let us run first the original version\n",
    "x1,_ = gradient_descent(f, Df, 1, α = 0.1, maxiter = 10000);\n",
    "\n",
    "# And then the new version\n",
    "x2, _ = gradient_descent(f, Df, α = 0.1, maxiter = 10000);\n",
    "\n",
    "println(\"The difference between the two solutions is $(abs(x1-x2)).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen above, both versions of `gradient_descent` coexist despite having used the same name. In fact, in Julia a function can have multiple *methods*, depending on which the inputs are. Thus, our `gradient_descent` has two methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods(gradient_descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other functions, like the `sort` function, have a lot more methods defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods(sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may guess from this list, the function `sort` has methods defined for a lot of relevant `types`: arrays, sparse arrays, ranges... And this is something we can also do by just annotating the type of the function's input. This adds even more flexibility: not only can we have different methods for different number of inputs, we can also have different methods for different *types* of the input, **and all behind the same function name**. Having different methods for different types helps Julia to compile the program in the most efficient way, which translates its impressive performance.\n",
    "\n",
    "On the other hand, this *multiple dispatch* also opens the door to reuse names of functions from standard libraries for other types (even user defined!) if they perform a similar task. For example, Julia doesn't provide a method for sorting a tuple (and this may make sense, since tuples can contain elements of any type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = (1, 3, 2)\n",
    "sort(A) # this doesn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we may as well write a method for sorting tuples that will get incorporated to the `sort` function methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base: sort                      # we must explicitly import the function to extend it\n",
    "sort(A::Tuple) = Base.sort(collect(A)) # here ::Tuple signals that the method is supposed to work on tuples\n",
    "methods(sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
