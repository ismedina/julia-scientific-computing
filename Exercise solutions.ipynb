{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Programming Gradient Descent in 1D\n",
    "\n",
    "#### Exercise 1:\n",
    "\n",
    "Improve the `gradient_descent` functions by adding:\n",
    "\n",
    "* *a tolerance and a maximum number of iterations*: stop the iterations only when `Df(x)` is smaller than the tolerance or the maximum number of iterations has been reached.\n",
    "* *progress info*: print the current number of iterations, the value of `x` and that of `f(x)` every 100 iterations (*Hint*: in the second code cell you can see how to format variables into text) \n",
    "* *a `verbose` parameter*, to activate or deactivate the showing of progress info.\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Third version of the gradient descent. Stops when the gradient is smaller than `TOL`, \n",
    "    or when the maximum number of iterations `maxiter` has been reached\"\"\"\n",
    "function gradient_descent(f, Df, x; alpha = 0.1, TOL = 1e-10, maxiter = 1000, verbose = false)\n",
    "    \n",
    "    N_iter = 0\n",
    "    grad = Df(x)\n",
    "    while (N_iter < maxiter) & (abs(grad) > TOL)\n",
    "        x = x - alpha*grad\n",
    "        grad = Df(x)\n",
    "        N_iter += 1\n",
    "        \n",
    "        if (N_iter % 100 == 0) & (verbose == true) # print progress\n",
    "            fx = f(x)\n",
    "            println(\"Iter. $N_iter,\\tx = $x,\\tf(x) = $fx\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (x, f(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Arrays, packages and Multidimensional Gradient Descent\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Build the function $g(x) = x_1^2 + 2x_2^2$ and its gradient using only matrix and scalar-matrix operations (Hint: the transpose of the vector `x` is `x'`).\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2(x) = x' * [1 0; 0 2] * x\n",
    "Dg2(x) = 2 .* [1 0; 0 2] * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Bring the gradient descent of the first notebook into the multidimensional realm, and make it output the _history_ of `x`s and `f(x)`s (Hint: consider using the concatenate functions that were explained above. You may also use `push!`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Gradient descent for multidimensional functions Stops when the gradient is smaller than `TOL`, \n",
    "    or when the maximum number of iterations `maxiter` has been reached\"\"\"\n",
    "function gradient_descent(f, Df, x; alpha = 0.1, TOL = 1e-10, maxiter = 1000, verbose = false)\n",
    "    N_iter = 0\n",
    "    grad = Df(x)\n",
    "    \n",
    "    xn = x\n",
    "    fn = [f(x)]\n",
    "    while (N_iter < maxiter) & (norm(grad) > TOL)\n",
    "        x = x - alpha*grad\n",
    "        \n",
    "        xn = [xn x]\n",
    "        fn = [fn f(x)]\n",
    "        \n",
    "        grad = Df(x)\n",
    "        N_iter += 1\n",
    "        \n",
    "        if (N_iter % 100 == 0) & (verbose == true)# print progress\n",
    "            fx = fn[end]\n",
    "            println(\"Iter. $N_iter,\\tx = $x,\\tf(x) = $fx\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (xn, fn)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
